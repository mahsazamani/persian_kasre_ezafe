{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqLaaXrhjodD",
        "outputId": "5bc6f3ac-ff8d-474d-a445-61fd39afc52e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.2)\n",
            "Requirement already satisfied: flashtext<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from hazm) (2.7)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.24.3)\n",
            "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.11)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.5.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (75.1.0)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.0.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.16.0)\n",
            "Collecting seqeval\n",
            "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.24.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=069321f73fcf65e2cc7721d35613e2f4b90809ef0e8d38a15e5ada8f892eadd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "#!pip install parsivar\n",
        "#!pip install libwapiti\n",
        "!pip install hazm\n",
        "!pip install seqeval\n",
        "#!pip install spacy_huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j33F-y07Zn1Y",
        "outputId": "7973d6c4-b968-45c4-a112-5a1573cc8369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbelzsO1ZEEM",
        "outputId": "f26e4f46-a42e-4e4f-8045-63a8e64133c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnUU64Khjw15",
        "outputId": "7cc71eea-7d07-425f-c99e-d01ae15b34a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('مشهد', 'N_SING'), ('به', 'P'), ('واسطه', 'N_SING'), ('وجود', 'N_SING'), ('حرم', 'N_SING'), ('مطهر', 'ADJ'), ('امام', 'N_SING'), ('رضا', 'N_SING'), ('ع', 'N_SING'), ('و', 'CON'), ('تولیتش،', 'N_SING'), ('پتانسیل', 'N_SING'), ('کمنظیری', 'N_SING'), ('دارد', 'V_PRS'), ('که', 'CON'), ('بعد', 'ADV_TIME'), ('از', 'P'), ('تهران', 'N_SING'), ('این', 'DET'), ('شهر', 'N_SING'), ('را', 'CLITIC'), ('به', 'P'), ('دومین', 'ADJ_SUP'), ('شهر', 'N_SING'), ('مستعد', 'ADJ'), ('در', 'P'), ('شب', 'N_SING')]\n"
          ]
        }
      ],
      "source": [
        "from parsivar import Tokenizer,Normalizer,POSTagger\n",
        "my_normalizer = Normalizer()\n",
        "my_tokenizer = Tokenizer()\n",
        "my_tagger = POSTagger(tagging_model=\"wapiti\")  # tagging_model = \"wapiti\" or \"stanford\". \"wapiti\" is faster than \"stanford\"\n",
        "text_tags = my_tagger.parse(my_tokenizer.tokenize_words(\"مشهد به واسطه وجود حرم مطهر امام رضا ع و تولیتش، پتانسیل کمنظیری دارد که بعد از تهران این شهر را به دومین شهر مستعد در شب\"))\n",
        "print(text_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCVozg0_Lw1S",
        "outputId": "576e736c-d917-4e89-de94-4c677c542a40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('مشهد', 'NOUN'), ('به', 'ADP'), ('واسطه', 'NOUN,EZ'), ('وجود', 'NOUN,EZ'), ('حرم', 'NOUN,EZ'), ('مطهر', 'ADJ,EZ'), ('امام', 'NOUN'), ('رضا', 'NOUN'), ('ع', 'NOUN'), ('و', 'CCONJ'), ('تولیتش', 'NOUN'), ('،', 'PUNCT'), ('پتانسیل', 'NOUN,EZ'), ('کمنظیری', 'NOUN'), ('دارد', 'VERB'), ('که', 'SCONJ'), ('بعد', 'ADP'), ('از', 'ADP'), ('تهران', 'NOUN'), ('این', 'DET'), ('شهر', 'NOUN'), ('را', 'ADP'), ('به', 'ADP'), ('دومین', 'NUM'), ('شهر', 'NOUN,EZ'), ('مستعد', 'ADJ'), ('در', 'ADP'), ('شب', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ],
      "source": [
        "from hazm import POSTagger,word_tokenize\n",
        "tagger = POSTagger(model='/content/drive/MyDrive/Kasre_Ez/pos_tagger.model')\n",
        "text_tags = tagger.tag(word_tokenize('مشهد به واسطه وجود حرم مطهر امام رضا ع و تولیتش، پتانسیل کمنظیری دارد که بعد از تهران این شهر را به دومین شهر مستعد در شب.'))\n",
        "print(text_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sWW9B_rNdaDI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertConfig, BertForTokenClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSL9CvZ48Pip",
        "outputId": "eb1a80ba-91e2-45e6-bdf4-009b7c672683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              sentence  \\\n",
            "0    بولتونبهتر است مادورو از پیشنهاد عفو عمومی ریی...   \n",
            "1    والا تو دانشگاه که افتادم باهاشون ادمای خوبی ب...   \n",
            "2    کلی شهید دادیم برای آزادی و استقلال این کشور، ...   \n",
            "3    بنظرتون کسی که از فیلمای نولان خوشش نیومده رو ...   \n",
            "4                                ولایت بغلان افغانستان   \n",
            "..                                                 ...   \n",
            "685  دادستان کل کشور هر گوشی تلفن همراه، یک کانال ا...   \n",
            "686                   رای ندادن بزرگترین اشتباه ممکن ه   \n",
            "687  شاید خانم فاطمه معتمداریا تاریخ را مطالعه نکرد...   \n",
            "688  یکیو دیدم امروزهی این بطری رو تکون میداداون زب...   \n",
            "689  واکنش روسیه به راه‌اندازی کانال ویژه تجارت اتح...   \n",
            "\n",
            "                                           word_labels  \n",
            "0    O,O,O,O,B-WORD,B-WORD,B-WORD,B-WORD,O,O,O,B-WO...  \n",
            "1    O,O,O,O,O,O,B-WORD,O,O,O,O,O,O,O,O,O,O,O,O,O,O...  \n",
            "2    O,O,O,B-WORD,O,O,B-WORD,O,O,O,B-WORD,O,B-WORD,...  \n",
            "3                         O,O,O,O,B-WORD,O,O,O,O,O,O,O  \n",
            "4                                      B-WORD,B-WORD,O  \n",
            "..                                                 ...  \n",
            "685  B-WORD,B-WORD,O,O,B-WORD,B-WORD,O,O,O,O,B-WORD...  \n",
            "686                                   O,O,O,B-WORD,O,O  \n",
            "687  O,B-WORD,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O...  \n",
            "688  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-WORD,O,O,O...  \n",
            "689  B-WORD,O,O,B-WORD,B-WORD,B-WORD,B-WORD,B-WORD,...  \n",
            "\n",
            "[690 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "df = pd.read_csv('/content/lscp-0.5-fa-newdata_POS - Sheet1.csv')\n",
        "\n",
        "def create_labels(text, selected_words):\n",
        "    if not isinstance(text, str):\n",
        "        text = ''  # Convert non-string text to an empty string\n",
        "\n",
        "    if not isinstance(selected_words, str):\n",
        "        selected_words = ''\n",
        "\n",
        "    selected_word_list = selected_words.split('،')\n",
        "\n",
        "    tokenized_input = text.split()\n",
        "\n",
        "    # Initialize labels as 'O' for every token\n",
        "    labels = ['O'] * len(tokenized_input)\n",
        "\n",
        "    for word in selected_word_list:\n",
        "        word_tokens = word.split()\n",
        "        n = len(word_tokens)\n",
        "        for i in range(len(tokenized_input) - n + 1):\n",
        "            if tokenized_input[i:i+n] == word_tokens:\n",
        "                try:\n",
        "                    labels[i] = 'B-WORD'\n",
        "                except:\n",
        "                  print('word_tokens',word_tokens)\n",
        "\n",
        "                  print('tokenized_input',text)\n",
        "\n",
        "                for j in range(1, n):\n",
        "                    labels[i+j] = 'I-WORD'\n",
        "                break\n",
        "\n",
        "    return labels\n",
        "output_data = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    sentence = row['text_query']\n",
        "    selected = row['selected_words']\n",
        "\n",
        "    labels = create_labels(sentence, selected)\n",
        "\n",
        "    labels_str = ','.join(labels)\n",
        "\n",
        "    output_data.append({'sentence': sentence, 'word_labels': labels_str})\n",
        "\n",
        "output_df = pd.DataFrame(output_data)\n",
        "print(output_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wSehB_5_8GO",
        "outputId": "cd239814-3a77-4f90-8e46-46680cd1ba32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "طرف میگ اگه یه قدم به بهشت باشم صدات از جهنم بیاد برمیگردم جون مادرت ترافیک درست نکن\n",
            "O,O,O,O,O,O,O,O,O,O,O,O,O,B-WORD,O,O,O,O\n"
          ]
        }
      ],
      "source": [
        "data = output_df[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
        "data.head()\n",
        "\n",
        "print(data.iloc[41].sentence)\n",
        "print(data.iloc[41].word_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AljR_IHcBWbz"
      },
      "outputs": [],
      "source": [
        "label2id={'I-WORD':2,'B-WORD': 1,'O': 0}\n",
        "id2label={2:'I-WORD',1:'B-WORD',0:'O'}\n",
        "#tokenizer = BertTokenizer.from_pretrained('HooshvareLab/bert-fa-base-uncased')\n",
        "\n",
        "\n",
        "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
        "    \"\"\"\n",
        "    Word piece tokenization makes it difficult to match word labels\n",
        "    back up with individual word pieces. This function tokenizes each\n",
        "    word one at a time so that it is easier to preserve the correct\n",
        "    label for each subword. It is, of course, a bit slower in processing\n",
        "    time, but it will help our model achieve higher accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    if isinstance(sentence, float):\n",
        "        sentence = str(sentence)  # Convert float to string\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    # Check if text_labels is empty or malformed\n",
        "    if not text_labels or len(sentence.split()) != len(text_labels.split(\",\")):\n",
        "        return None, None  # Indicate that this sample should be skipped\n",
        "\n",
        "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels\n",
        "\n",
        "class dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence = self.data.sentence[index]\n",
        "        word_labels = self.data.word_labels[index]\n",
        "\n",
        "        # Tokenization process\n",
        "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
        "\n",
        "        # Ensure tokenized_sentence and labels are valid\n",
        "        if tokenized_sentence is None or labels is None:\n",
        "            print(f\"Invalid tokenized sentence or labels at index {index}. Skipping entry.\")\n",
        "            return self.__getitem__((index + 1) % self.len)  # Skip and try the next index\n",
        "\n",
        "        try:\n",
        "            # Step 2: Add special tokens and corresponding labels\n",
        "            tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"]\n",
        "            labels = [\"O\"] + labels + [\"O\"]\n",
        "\n",
        "            # Step 3: Truncate or pad the tokenized sentence and labels to max length\n",
        "            maxlen = self.max_len\n",
        "            if len(tokenized_sentence) > maxlen:\n",
        "                tokenized_sentence = tokenized_sentence[:maxlen]\n",
        "                labels = labels[:maxlen]\n",
        "            else:\n",
        "                tokenized_sentence += ['[PAD]'] * (maxlen - len(tokenized_sentence))\n",
        "                labels += [\"O\"] * (maxlen - len(labels))\n",
        "\n",
        "            # Step 4: Create attention mask\n",
        "            attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
        "\n",
        "            # Step 5: Convert tokens to input IDs\n",
        "            ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "\n",
        "            # Step 6: Convert labels to IDs\n",
        "            label_ids = [label2id[label] for label in labels]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing index {index}: {e}\")\n",
        "            return self.__getitem__((index + 1) % self.len)  # Skip the current entry\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(label_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUwSSbPQCoex",
        "outputId": "b54843bc-6c96-4bb7-b21b-7df068dff6ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL Dataset: (686, 2)\n",
            "TRAIN Dataset: (549, 2)\n",
            "TEST Dataset: (137, 2)\n"
          ]
        }
      ],
      "source": [
        "train_size = 0.8\n",
        "MAX_LEN = 128\n",
        "train_dataset = data.sample(frac=train_size,random_state=200)\n",
        "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(data.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BFIy-ZBCti8",
        "outputId": "9504b942-c7a5-497c-8b06-761773026ebb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': tensor([    2, 29343, 82505,  2806, 28804,  2791,  4360, 14682,  3885,  4800,\n",
              "          3590,  2988,  3054,  1012,  2831, 28804,  1379, 53061,  9161,  4357,\n",
              "          1379,  4977,  2803,  2786,  2829,  6432,  4357,  2786,  4645,  3021,\n",
              "          2791, 11226, 12988,  5165,  1012,     4,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
              " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'targets': tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "         0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "testing_set[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lDtmadnC53l",
        "outputId": "d494290a-3522-4ccb-b4ec-0b550ae4c411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]       O\n",
            "بچه         O\n",
            "که          O\n",
            "بودم        O\n",
            "لحظه        O\n",
            "##شماری     O\n",
            "میکردم      O\n",
            "ظهر         O\n",
            "بشه         O\n",
            "ناهار       O\n",
            "##و         O\n",
            "بخورم       O\n",
            "و           O\n",
            "برم         O\n",
            "بیرون       O\n",
            "بازی        O\n",
            "کنم         O\n",
            "با          O\n",
            "دوستام      O\n",
            "و           O\n",
            "بچههای      B-WORD\n",
            "کوچههای     B-WORD\n",
            "بق          O\n",
            "##لی        O\n",
            "،           O\n",
            "تو          O\n",
            "زل          B-WORD\n",
            "گرما        O\n",
            "[SEP]       O\n",
            "[PAD]       O\n"
          ]
        }
      ],
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"ids\"][:30]), training_set[0][\"targets\"][:30]):\n",
        "  print('{0:10}  {1}'.format(token, id2label[label.item()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZMyhIv44IjQP"
      },
      "outputs": [],
      "source": [
        "#MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 16\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mGGSmYHMIcNR"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeM7CNRwInom",
        "outputId": "471c935c-364e-4ce9-afd8-ea3615f6fc53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(100000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model = BertForTokenClassification.from_pretrained('HooshvareLab/bert-fa-base-uncased',\n",
        "#                                                    num_labels=len(id2label),\n",
        "#                                                    id2label=id2label,\n",
        "#                                                    label2id=label2id)\n",
        "\n",
        "# model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbXPTNeuKLn-",
        "outputId": "8ac86733-9dcf-4e43-d4ef-11c0350ce2ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1341, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "#device = 'cuda'\n",
        "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
        "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
        "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
        "ids = ids.to(device)\n",
        "mask = mask.to(device)\n",
        "targets = targets.to(device)\n",
        "outputs = model(input_ids=ids.to(\"cuda\"), attention_mask=mask.to(\"cuda\"), labels=targets.to(\"cuda\"))\n",
        "initial_loss = outputs[0]\n",
        "initial_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pjLwttAKtsv",
        "outputId": "a2021662-d109-4a81-9e78-4e3901c46e7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "tr_logits = outputs[1]\n",
        "tr_logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xHUVXO3iLJxH"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUHeNcXYLPRe",
        "outputId": "f6bbc87a-5f6c-4de1-a867-1bf76522102f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 1.1768450736999512\n",
            "Training loss per 100 training steps: 0.13366310731830575\n",
            "Training loss per 100 training steps: 0.09513311094571998\n",
            "Training loss per 100 training steps: 0.07947372157819742\n",
            "Training loss per 100 training steps: 0.06943823001405544\n",
            "Training loss per 100 training steps: 0.06317980769657328\n",
            "Training loss per 100 training steps: 0.05894461352712848\n",
            "Training loss per 100 training steps: 0.055862814438437644\n",
            "Training loss per 100 training steps: 0.05302286856911249\n",
            "Training loss per 100 training steps: 0.050898635550634674\n",
            "Training loss per 100 training steps: 0.04904259838857925\n",
            "Training loss per 100 training steps: 0.04749295397701383\n",
            "Training loss per 100 training steps: 0.045997574451619096\n",
            "Training loss per 100 training steps: 0.04472272224376389\n",
            "Training loss per 100 training steps: 0.043468505432955565\n",
            "Training loss per 100 training steps: 0.04238247493456312\n",
            "Training loss per 100 training steps: 0.0414810910277641\n",
            "Training loss per 100 training steps: 0.040720684134096415\n",
            "Training loss per 100 training steps: 0.039952202300866366\n",
            "Training loss per 100 training steps: 0.03929176431164921\n",
            "Training loss per 100 training steps: 0.038696490244117046\n",
            "Training loss per 100 training steps: 0.038026690044028526\n",
            "Training loss per 100 training steps: 0.03746199266991829\n",
            "Training loss per 100 training steps: 0.03695990265669242\n",
            "Training loss per 100 training steps: 0.03636840940816759\n",
            "Training loss per 100 training steps: 0.03590747074773129\n",
            "Training loss per 100 training steps: 0.035545246261778914\n",
            "Training loss per 100 training steps: 0.035131179794161596\n",
            "Training loss per 100 training steps: 0.034673967181416666\n",
            "Training loss per 100 training steps: 0.03430746997437244\n",
            "Training loss per 100 training steps: 0.033869264322829266\n",
            "Training loss per 100 training steps: 0.03346951109548188\n",
            "Training loss per 100 training steps: 0.03312288736633582\n",
            "Training loss per 100 training steps: 0.032905362737589365\n",
            "Training loss per 100 training steps: 0.032565287396575195\n",
            "Training loss per 100 training steps: 0.0322890681335934\n",
            "Training loss per 100 training steps: 0.03192274068803955\n",
            "Training loss per 100 training steps: 0.03159243100820025\n",
            "Training loss per 100 training steps: 0.03133982595751418\n",
            "Training loss per 100 training steps: 0.03109915598636549\n",
            "Training loss per 100 training steps: 0.030811269991334933\n",
            "Training loss per 100 training steps: 0.03061130423050204\n",
            "Training loss per 100 training steps: 0.03046977246062459\n",
            "Training loss per 100 training steps: 0.030194418620356284\n",
            "Training loss per 100 training steps: 0.030004767395561897\n",
            "Training loss per 100 training steps: 0.02982655179673809\n",
            "Training loss per 100 training steps: 0.02970773297561613\n",
            "Training loss per 100 training steps: 0.02953039014898102\n",
            "Training loss per 100 training steps: 0.02938898782075787\n",
            "Training loss per 100 training steps: 0.0291692814115443\n",
            "Training loss per 100 training steps: 0.029022594043259818\n",
            "Training loss per 100 training steps: 0.028847802501969846\n",
            "Training loss per 100 training steps: 0.028710410672280046\n",
            "Training loss per 100 training steps: 0.028557311021603714\n",
            "Training loss per 100 training steps: 0.02841957974580122\n",
            "Training loss per 100 training steps: 0.028303032555214606\n",
            "Training loss per 100 training steps: 0.028208665607398972\n",
            "Training loss per 100 training steps: 0.02812556893842179\n",
            "Training loss per 100 training steps: 0.027987396835758068\n",
            "Training loss per 100 training steps: 0.027855905963296367\n",
            "Training loss per 100 training steps: 0.027763965625289502\n",
            "Training loss per 100 training steps: 0.027683528937909374\n",
            "Training loss per 100 training steps: 0.027602986056977283\n",
            "Training loss per 100 training steps: 0.02750559658860346\n",
            "Training loss per 100 training steps: 0.02737329748926413\n",
            "Training loss per 100 training steps: 0.027246929948375517\n",
            "Training loss per 100 training steps: 0.02711390869051137\n",
            "Training loss per 100 training steps: 0.02702058333694825\n",
            "Training loss per 100 training steps: 0.02691732426450559\n",
            "Training loss per 100 training steps: 0.026855590169888054\n",
            "Training loss per 100 training steps: 0.026747399695828487\n",
            "Training loss per 100 training steps: 0.026649140221165022\n",
            "Training loss per 100 training steps: 0.026564663422749355\n",
            "Training loss per 100 training steps: 0.02648956814602031\n",
            "Training loss per 100 training steps: 0.026403372765754055\n",
            "Training loss per 100 training steps: 0.026326268705972305\n",
            "Training loss per 100 training steps: 0.026262537005149595\n",
            "Training loss per 100 training steps: 0.026183499005300825\n",
            "Training loss epoch: 0.0261348150214793\n",
            "Training accuracy epoch: 0.9332101148067913\n"
          ]
        }
      ],
      "source": [
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "        loss, tr_logits = outputs.loss, outputs.logits\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "\n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "\n",
        "        # compute training accuracy\n",
        "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        active_accuracy = mask.view(-1) == 1 # shape (batch_size * seq_len,)\n",
        "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "        tr_preds.extend(predictions)\n",
        "        tr_labels.extend(targets)\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEWMF9PlLXmf",
        "outputId": "7278f748-dc75-4c38-afd8-ab656b0f5e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss per 100 evaluation steps: 0.044483527541160583\n",
            "Validation Loss: 0.05665095440215535\n",
            "Validation Accuracy: 0.8850574108607108\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            if batch is None:  # Skip invalid batches\n",
        "                continue\n",
        "\n",
        "            ids = batch['ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['mask'].to(device, dtype=torch.long)\n",
        "            targets = batch['targets'].to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss, eval_logits = outputs.loss, outputs.logits\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += targets.size(0)\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                print(f\"Validation loss per 100 evaluation steps: {eval_loss / nb_eval_steps}\")\n",
        "\n",
        "            # Compute accuracy\n",
        "            flattened_targets = targets.view(-1)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1)\n",
        "            active_accuracy = mask.view(-1) == 1\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "            eval_labels.extend(targets)\n",
        "            eval_preds.extend(predictions)\n",
        "\n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    eval_loss /= nb_eval_steps\n",
        "    eval_accuracy /= nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return eval_labels, eval_preds\n",
        "labels, predictions = valid(model, testing_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmoZ9WWIXXCR"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters(): param.data = param.data.contiguous()\n",
        "model.save_pretrained('/content/drive/MyDrive/Kasre_Ez/models_5000')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpaVXqFGhBXL",
        "outputId": "0b8b3603-a2ae-438a-af64-1136b66074c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Kasre_Ez/models_5000/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Kasre_Ez/models_5000/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Kasre_Ez/models_5000/vocab.txt',\n",
              " '/content/drive/MyDrive/Kasre_Ez/models_5000/added_tokens.json',\n",
              " '/content/drive/MyDrive/Kasre_Ez/models_5000/tokenizer.json')"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.save_pretrained('/content/drive/MyDrive/Kasre_Ez/models_5000')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4XzTOppmjCDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366a69a7-ed0a-4faf-e64c-88418ffe8903"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(100000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "model_name = \"/content/drive/MyDrive/Kasre_Ez/models_folder\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DAB8AGkgkQf",
        "outputId": "d0907059-da34-49f0-c3ad-7cc7b08935c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertTokenizerFast(name_or_path='/content/drive/MyDrive/Kasre_Ez/models_folder', vocab_size=100000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
              "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t3: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t4: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oU41d4eWqPB",
        "outputId": "20c65470-ad16-4513-e1d0-32bc2ec3352c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        WORD       0.57      0.55      0.56       361\n",
            "\n",
            "   micro avg       0.57      0.55      0.56       361\n",
            "   macro avg       0.57      0.55      0.56       361\n",
            "weighted avg       0.57      0.55      0.56       361\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from seqeval.metrics import classification_report\n",
        "\n",
        "labels = [label.item() for label in labels]  # If labels are tensors\n",
        "predictions = [prediction.item() for prediction in predictions]  # If predictions are tensors\n",
        "\n",
        "labels = [id2label[label_id] for label_id in labels]  # Mapping label IDs to label names\n",
        "predictions = [id2label[pred_id] for pred_id in predictions]  # Mapping prediction IDs to label names\n",
        "\n",
        "print(classification_report([labels], [predictions]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jfwLkKVSrNir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fine_tune Again"
      ],
      "metadata": {
        "id": "P0x1ukcJrRxD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d65b91dd-55f5-45da-cbd2-66fc048cf7db",
        "id": "o-2FDZ2uyK50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.05642840638756752\n",
            "Training loss per 100 training steps: 0.03182098453306584\n",
            "Training loss epoch: 0.02868368513831545\n",
            "Training accuracy epoch: 0.9335554837583694\n"
          ]
        }
      ],
      "source": [
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "        loss, tr_logits = outputs.loss, outputs.logits\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "\n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "\n",
        "        # compute training accuracy\n",
        "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        active_accuracy = mask.view(-1) == 1 # shape (batch_size * seq_len,)\n",
        "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "        tr_preds.extend(predictions)\n",
        "        tr_labels.extend(targets)\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oPkUxedQyMEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dfd029e-a5b2-48ee-9524-3c92511efac3",
        "id": "7gMCR_PWybsk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss per 100 evaluation steps: 0.025118211284279823\n",
            "Validation Loss: 0.021392777148220275\n",
            "Validation Accuracy: 0.9482117321364465\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            if batch is None:  # Skip invalid batches\n",
        "                continue\n",
        "\n",
        "            ids = batch['ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['mask'].to(device, dtype=torch.long)\n",
        "            targets = batch['targets'].to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss, eval_logits = outputs.loss, outputs.logits\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += targets.size(0)\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                print(f\"Validation loss per 100 evaluation steps: {eval_loss / nb_eval_steps}\")\n",
        "\n",
        "            # Compute accuracy\n",
        "            flattened_targets = targets.view(-1)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1)\n",
        "            active_accuracy = mask.view(-1) == 1\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "            eval_labels.extend(targets)\n",
        "            eval_preds.extend(predictions)\n",
        "\n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    eval_loss /= nb_eval_steps\n",
        "    eval_accuracy /= nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return eval_labels, eval_preds\n",
        "labels, predictions = valid(model, testing_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "569e05e9-97c2-4554-c603-4a4aa59b705a",
        "id": "FyUeE5_RykSg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        WORD       0.77      0.86      0.81       361\n",
            "\n",
            "   micro avg       0.77      0.86      0.81       361\n",
            "   macro avg       0.77      0.86      0.81       361\n",
            "weighted avg       0.77      0.86      0.81       361\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#results of finetuned twice model\n",
        "from seqeval.metrics import classification_report\n",
        "labels = [label.item() for label in labels]  # If labels are tensors\n",
        "predictions = [prediction.item() for prediction in predictions]  # If predictions are tensors\n",
        "\n",
        "labels = [id2label[label_id] for label_id in labels]  # Mapping label IDs to label names\n",
        "predictions = [id2label[pred_id] for pred_id in predictions]  # Mapping prediction IDs to label names\n",
        "\n",
        "print(classification_report([labels], [predictions]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters(): param.data = param.data.contiguous()\n",
        "model.save_pretrained('/content/drive/MyDrive/Kasre_Ez/models_finetuned_twice')"
      ],
      "metadata": {
        "id": "SwwwUpEtrjJy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained('/content/drive/MyDrive/Kasre_Ez/models_finetuned_twice')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jck_FWsB0F7b",
        "outputId": "c6559ab5-ac76-4f0e-be56-f4c759f6f572"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Kasre_Ez/models_finetuned_twice/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Kasre_Ez/models_finetuned_twice/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Kasre_Ez/models_finetuned_twice/vocab.txt',\n",
              " '/content/drive/MyDrive/Kasre_Ez/models_finetuned_twice/added_tokens.json',\n",
              " '/content/drive/MyDrive/Kasre_Ez/models_finetuned_twice/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LuzJG2T_0LyM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}