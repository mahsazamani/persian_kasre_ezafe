{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqLaaXrhjodD",
        "outputId": "2322e9e6-6ab8-4ad0-8a6c-59a8444ed501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
            "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.2)\n",
            "Requirement already satisfied: flashtext<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from hazm) (2.7)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Requirement already satisfied: numpy==1.24.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.24.3)\n",
            "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.11)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.5.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext-wheel<0.10.0,>=0.9.2->hazm) (71.0.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (7.0.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.66.5)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim<5.0.0,>=4.3.1->hazm) (1.16.0)\n",
            "Collecting seqeval\n",
            "  Using cached seqeval-1.2.2.tar.gz (43 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.24.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=b472ac6c66f253c0ce065fbe0b395b11f2f42830912f7fd27c377939ca420fcd\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "#!pip install parsivar\n",
        "#!pip install libwapiti\n",
        "!pip install hazm\n",
        "!pip install seqeval\n",
        "#!pip install spacy_huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j33F-y07Zn1Y",
        "outputId": "2da77db2-f179-45bc-eb39-03fe841de85f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbelzsO1ZEEM",
        "outputId": "6e3cdbd7-7db0-4d16-931c-f8b5a1b9bade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnUU64Khjw15",
        "outputId": "7cc71eea-7d07-425f-c99e-d01ae15b34a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('مشهد', 'N_SING'), ('به', 'P'), ('واسطه', 'N_SING'), ('وجود', 'N_SING'), ('حرم', 'N_SING'), ('مطهر', 'ADJ'), ('امام', 'N_SING'), ('رضا', 'N_SING'), ('ع', 'N_SING'), ('و', 'CON'), ('تولیتش،', 'N_SING'), ('پتانسیل', 'N_SING'), ('کمنظیری', 'N_SING'), ('دارد', 'V_PRS'), ('که', 'CON'), ('بعد', 'ADV_TIME'), ('از', 'P'), ('تهران', 'N_SING'), ('این', 'DET'), ('شهر', 'N_SING'), ('را', 'CLITIC'), ('به', 'P'), ('دومین', 'ADJ_SUP'), ('شهر', 'N_SING'), ('مستعد', 'ADJ'), ('در', 'P'), ('شب', 'N_SING')]\n"
          ]
        }
      ],
      "source": [
        "from parsivar import Tokenizer,Normalizer,POSTagger\n",
        "my_normalizer = Normalizer()\n",
        "my_tokenizer = Tokenizer()\n",
        "my_tagger = POSTagger(tagging_model=\"wapiti\")  # tagging_model = \"wapiti\" or \"stanford\". \"wapiti\" is faster than \"stanford\"\n",
        "text_tags = my_tagger.parse(my_tokenizer.tokenize_words(\"مشهد به واسطه وجود حرم مطهر امام رضا ع و تولیتش، پتانسیل کمنظیری دارد که بعد از تهران این شهر را به دومین شهر مستعد در شب\"))\n",
        "print(text_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCVozg0_Lw1S",
        "outputId": "576e736c-d917-4e89-de94-4c677c542a40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('مشهد', 'NOUN'), ('به', 'ADP'), ('واسطه', 'NOUN,EZ'), ('وجود', 'NOUN,EZ'), ('حرم', 'NOUN,EZ'), ('مطهر', 'ADJ,EZ'), ('امام', 'NOUN'), ('رضا', 'NOUN'), ('ع', 'NOUN'), ('و', 'CCONJ'), ('تولیتش', 'NOUN'), ('،', 'PUNCT'), ('پتانسیل', 'NOUN,EZ'), ('کمنظیری', 'NOUN'), ('دارد', 'VERB'), ('که', 'SCONJ'), ('بعد', 'ADP'), ('از', 'ADP'), ('تهران', 'NOUN'), ('این', 'DET'), ('شهر', 'NOUN'), ('را', 'ADP'), ('به', 'ADP'), ('دومین', 'NUM'), ('شهر', 'NOUN,EZ'), ('مستعد', 'ADJ'), ('در', 'ADP'), ('شب', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ],
      "source": [
        "from hazm import POSTagger,word_tokenize\n",
        "tagger = POSTagger(model='/content/drive/MyDrive/Kasre_Ez/pos_tagger.model')\n",
        "text_tags = tagger.tag(word_tokenize('مشهد به واسطه وجود حرم مطهر امام رضا ع و تولیتش، پتانسیل کمنظیری دارد که بعد از تهران این شهر را به دومین شهر مستعد در شب.'))\n",
        "print(text_tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWW9B_rNdaDI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertConfig, BertForTokenClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSL9CvZ48Pip",
        "outputId": "6de2dfe3-c991-4330-f47a-78159762a3c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "49996\n",
            "word_tokens []\n",
            "tokenized_input \n",
            "word_tokens []\n",
            "tokenized_input \n",
            "word_tokens []\n",
            "tokenized_input \n",
            "word_tokens []\n",
            "tokenized_input \n",
            "word_tokens []\n",
            "tokenized_input \n",
            "word_tokens []\n",
            "tokenized_input \n",
            "word_tokens []\n",
            "tokenized_input \n",
            "word_tokens []\n",
            "tokenized_input \n",
            "word_tokens []\n",
            "tokenized_input \n",
            "word_tokens []\n",
            "tokenized_input \n",
            "word_tokens []\n",
            "tokenized_input \n",
            "word_tokens []\n",
            "tokenized_input \n",
            "word_tokens []\n",
            "tokenized_input \n",
            "word_tokens []\n",
            "tokenized_input \n",
            "                                                sentence  \\\n",
            "0      بنر تصاویر دیکتاتور توسط کانونهایشورشی به آتش ...   \n",
            "1      مشهد به واسطه وجود حرم مطهر امام رضاع و تولیتش...   \n",
            "2      عاخه تو مغزشون کردن هرکسی که مثلا از نظام انتق...   \n",
            "3                                         نرم باشه لطفا.   \n",
            "4                          ولی پولشو می‌داد راضی‌تر بودم   \n",
            "...                                                  ...   \n",
            "49991   این حد از بیشعوری رو با عکس ائمه نشون ندین داعشی   \n",
            "49992                                              به گا   \n",
            "49993                                    ممنونم عزیز دلم   \n",
            "49994  طبیعیه. به خاطر خاطرات بد گذشته. ولی یادت باشه...   \n",
            "49995  دل در هوس وصال او نیستاین است دلیل غیبت یار ال...   \n",
            "\n",
            "                                             word_labels  \n",
            "0                     B-WORD,B-WORD,O,B-WORD,O,O,O,O,O,O  \n",
            "1      O,O,B-WORD,B-WORD,B-WORD,B-WORD,O,O,O,O,B-WORD...  \n",
            "2      B-WORD,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O...  \n",
            "3                                             B-WORD,O,O  \n",
            "4                                         B-WORD,O,O,O,O  \n",
            "...                                                  ...  \n",
            "49991                    O,O,O,O,O,O,B-WORD,B-WORD,O,O,O  \n",
            "49992                                           B-WORD,O  \n",
            "49993                                         O,B-WORD,O  \n",
            "49994  O,O,B-WORD,B-WORD,B-WORD,O,O,O,O,O,O,O,O,O,O,O...  \n",
            "49995     O,O,B-WORD,B-WORD,O,O,O,B-WORD,B-WORD,B-WORD,O  \n",
            "\n",
            "[49996 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "df = pd.read_csv('/content/lscp-50000.csv')\n",
        "\n",
        "def create_labels(text, selected_words):\n",
        "    if not isinstance(text, str):\n",
        "        text = ''  # Convert non-string text to an empty string\n",
        "\n",
        "    if not isinstance(selected_words, str):\n",
        "        selected_words = ''\n",
        "\n",
        "    selected_word_list = selected_words.split('،')\n",
        "\n",
        "    tokenized_input = text.split()\n",
        "\n",
        "    # Initialize labels as 'O' for every token\n",
        "    labels = ['O'] * len(tokenized_input)\n",
        "\n",
        "    for word in selected_word_list:\n",
        "        word_tokens = word.split()\n",
        "        n = len(word_tokens)\n",
        "        for i in range(len(tokenized_input) - n + 1):\n",
        "            if tokenized_input[i:i+n] == word_tokens:\n",
        "                try:\n",
        "                    labels[i] = 'B-WORD'\n",
        "                except:\n",
        "                  print('word_tokens',word_tokens)\n",
        "\n",
        "                  print('tokenized_input',text)\n",
        "\n",
        "                for j in range(1, n):\n",
        "                    labels[i+j] = 'I-WORD'\n",
        "                break\n",
        "\n",
        "    return labels\n",
        "output_data = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    sentence = row['text_query']\n",
        "    selected = row['selected_words']\n",
        "\n",
        "    labels = create_labels(sentence, selected)\n",
        "\n",
        "    labels_str = ','.join(labels)\n",
        "\n",
        "    output_data.append({'sentence': sentence, 'word_labels': labels_str})\n",
        "\n",
        "output_df = pd.DataFrame(output_data)\n",
        "print(output_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wSehB_5_8GO",
        "outputId": "3edcb90c-99fe-4229-99fb-4af1f9e9b228"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "انگلیس خواستار تحریم‌های اتحادیه اروپا علیه مادورو شد\n",
            "O,B-WORD,B-WORD,B-WORD,O,B-WORD,O,O\n"
          ]
        }
      ],
      "source": [
        "data = output_df[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
        "data.head()\n",
        "\n",
        "print(data.iloc[41].sentence)\n",
        "print(data.iloc[41].word_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AljR_IHcBWbz",
        "outputId": "bc594ad7-76c4-46a2-a029-77416f072f6e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "label2id={'I-WORD':2,'B-WORD': 1,'O': 0}\n",
        "id2label={2:'I-WORD',1:'B-WORD',0:'O'}\n",
        "tokenizer = BertTokenizer.from_pretrained('HooshvareLab/bert-fa-base-uncased')\n",
        "\n",
        "\n",
        "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
        "    \"\"\"\n",
        "    Word piece tokenization makes it difficult to match word labels\n",
        "    back up with individual word pieces. This function tokenizes each\n",
        "    word one at a time so that it is easier to preserve the correct\n",
        "    label for each subword. It is, of course, a bit slower in processing\n",
        "    time, but it will help our model achieve higher accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    if isinstance(sentence, float):\n",
        "        sentence = str(sentence)  # Convert float to string\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    # Check if text_labels is empty or malformed\n",
        "    if not text_labels or len(sentence.split()) != len(text_labels.split(\",\")):\n",
        "        return None, None  # Indicate that this sample should be skipped\n",
        "\n",
        "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels\n",
        "\n",
        "class dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence = self.data.sentence[index]\n",
        "        word_labels = self.data.word_labels[index]\n",
        "\n",
        "        # Tokenization process\n",
        "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
        "\n",
        "        # Ensure tokenized_sentence and labels are valid\n",
        "        if tokenized_sentence is None or labels is None:\n",
        "            print(f\"Invalid tokenized sentence or labels at index {index}. Skipping entry.\")\n",
        "            return self.__getitem__((index + 1) % self.len)  # Skip and try the next index\n",
        "\n",
        "        try:\n",
        "            # Step 2: Add special tokens and corresponding labels\n",
        "            tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"]\n",
        "            labels = [\"O\"] + labels + [\"O\"]\n",
        "\n",
        "            # Step 3: Truncate or pad the tokenized sentence and labels to max length\n",
        "            maxlen = self.max_len\n",
        "            if len(tokenized_sentence) > maxlen:\n",
        "                tokenized_sentence = tokenized_sentence[:maxlen]\n",
        "                labels = labels[:maxlen]\n",
        "            else:\n",
        "                tokenized_sentence += ['[PAD]'] * (maxlen - len(tokenized_sentence))\n",
        "                labels += [\"O\"] * (maxlen - len(labels))\n",
        "\n",
        "            # Step 4: Create attention mask\n",
        "            attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
        "\n",
        "            # Step 5: Convert tokens to input IDs\n",
        "            ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "\n",
        "            # Step 6: Convert labels to IDs\n",
        "            label_ids = [label2id[label] for label in labels]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing index {index}: {e}\")\n",
        "            return self.__getitem__((index + 1) % self.len)  # Skip the current entry\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
        "            'targets': torch.tensor(label_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUwSSbPQCoex",
        "outputId": "d2e73a1d-b75b-4119-e563-360c4f3302eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FULL Dataset: (38659, 2)\n",
            "TRAIN Dataset: (30927, 2)\n",
            "TEST Dataset: (7732, 2)\n"
          ]
        }
      ],
      "source": [
        "train_size = 0.8\n",
        "MAX_LEN = 128\n",
        "train_dataset = data.sample(frac=train_size,random_state=200)\n",
        "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(data.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BFIy-ZBCti8",
        "outputId": "b55777ab-ad74-4b93-8cab-312de93f09fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': tensor([    2,  5574, 12139, 14744,  1012,     4,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
              " 'mask': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'targets': tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0])}"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testing_set[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lDtmadnC53l",
        "outputId": "d72612cb-59cc-4d20-9007-0219ce36ecb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS]       O\n",
            "کاری        O\n",
            "به          O\n",
            "این         O\n",
            "ندارم       O\n",
            "ولی         O\n",
            "کسی         O\n",
            "که          O\n",
            "شمارش       B-WORD\n",
            "##و         B-WORD\n",
            "میذاره      B-WORD\n",
            "پسو         B-WORD\n",
            "##ورد       B-WORD\n",
            "ساده        O\n",
            "لوحه        O\n",
            ".           O\n",
            "؟           O\n",
            "[SEP]       O\n",
            "[PAD]       O\n",
            "[PAD]       O\n",
            "[PAD]       O\n",
            "[PAD]       O\n",
            "[PAD]       O\n",
            "[PAD]       O\n",
            "[PAD]       O\n",
            "[PAD]       O\n",
            "[PAD]       O\n",
            "[PAD]       O\n",
            "[PAD]       O\n",
            "[PAD]       O\n"
          ]
        }
      ],
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"ids\"][:30]), training_set[0][\"targets\"][:30]):\n",
        "  print('{0:10}  {1}'.format(token, id2label[label.item()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMyhIv44IjQP"
      },
      "outputs": [],
      "source": [
        "#MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 16\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGGSmYHMIcNR"
      },
      "outputs": [],
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeM7CNRwInom",
        "outputId": "471c935c-364e-4ce9-afd8-ea3615f6fc53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(100000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 131,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = BertForTokenClassification.from_pretrained('HooshvareLab/bert-fa-base-uncased',\n",
        "                                                   num_labels=len(id2label),\n",
        "                                                   id2label=id2label,\n",
        "                                                   label2id=label2id)\n",
        "\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbXPTNeuKLn-",
        "outputId": "ceb1c982-5f5f-42f2-f26d-0dd997aec037"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.1718, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
        "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
        "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
        "ids = ids.to(device)\n",
        "mask = mask.to(device)\n",
        "targets = targets.to(device)\n",
        "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "initial_loss = outputs[0]\n",
        "initial_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pjLwttAKtsv",
        "outputId": "22b93dcd-e668-48b0-be6a-41fcd869b2bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 3])"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tr_logits = outputs[1]\n",
        "tr_logits.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHUVXO3iLJxH"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUHeNcXYLPRe",
        "outputId": "f6bbc87a-5f6c-4de1-a867-1bf76522102f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 1.1768450736999512\n",
            "Training loss per 100 training steps: 0.13366310731830575\n",
            "Training loss per 100 training steps: 0.09513311094571998\n",
            "Training loss per 100 training steps: 0.07947372157819742\n",
            "Training loss per 100 training steps: 0.06943823001405544\n",
            "Training loss per 100 training steps: 0.06317980769657328\n",
            "Training loss per 100 training steps: 0.05894461352712848\n",
            "Training loss per 100 training steps: 0.055862814438437644\n",
            "Training loss per 100 training steps: 0.05302286856911249\n",
            "Training loss per 100 training steps: 0.050898635550634674\n",
            "Training loss per 100 training steps: 0.04904259838857925\n",
            "Training loss per 100 training steps: 0.04749295397701383\n",
            "Training loss per 100 training steps: 0.045997574451619096\n",
            "Training loss per 100 training steps: 0.04472272224376389\n",
            "Training loss per 100 training steps: 0.043468505432955565\n",
            "Training loss per 100 training steps: 0.04238247493456312\n",
            "Training loss per 100 training steps: 0.0414810910277641\n",
            "Training loss per 100 training steps: 0.040720684134096415\n",
            "Training loss per 100 training steps: 0.039952202300866366\n",
            "Training loss per 100 training steps: 0.03929176431164921\n",
            "Training loss per 100 training steps: 0.038696490244117046\n",
            "Training loss per 100 training steps: 0.038026690044028526\n",
            "Training loss per 100 training steps: 0.03746199266991829\n",
            "Training loss per 100 training steps: 0.03695990265669242\n",
            "Training loss per 100 training steps: 0.03636840940816759\n",
            "Training loss per 100 training steps: 0.03590747074773129\n",
            "Training loss per 100 training steps: 0.035545246261778914\n",
            "Training loss per 100 training steps: 0.035131179794161596\n",
            "Training loss per 100 training steps: 0.034673967181416666\n",
            "Training loss per 100 training steps: 0.03430746997437244\n",
            "Training loss per 100 training steps: 0.033869264322829266\n",
            "Training loss per 100 training steps: 0.03346951109548188\n",
            "Training loss per 100 training steps: 0.03312288736633582\n",
            "Training loss per 100 training steps: 0.032905362737589365\n",
            "Training loss per 100 training steps: 0.032565287396575195\n",
            "Training loss per 100 training steps: 0.0322890681335934\n",
            "Training loss per 100 training steps: 0.03192274068803955\n",
            "Training loss per 100 training steps: 0.03159243100820025\n",
            "Training loss per 100 training steps: 0.03133982595751418\n",
            "Training loss per 100 training steps: 0.03109915598636549\n",
            "Training loss per 100 training steps: 0.030811269991334933\n",
            "Training loss per 100 training steps: 0.03061130423050204\n",
            "Training loss per 100 training steps: 0.03046977246062459\n",
            "Training loss per 100 training steps: 0.030194418620356284\n",
            "Training loss per 100 training steps: 0.030004767395561897\n",
            "Training loss per 100 training steps: 0.02982655179673809\n",
            "Training loss per 100 training steps: 0.02970773297561613\n",
            "Training loss per 100 training steps: 0.02953039014898102\n",
            "Training loss per 100 training steps: 0.02938898782075787\n",
            "Training loss per 100 training steps: 0.0291692814115443\n",
            "Training loss per 100 training steps: 0.029022594043259818\n",
            "Training loss per 100 training steps: 0.028847802501969846\n",
            "Training loss per 100 training steps: 0.028710410672280046\n",
            "Training loss per 100 training steps: 0.028557311021603714\n",
            "Training loss per 100 training steps: 0.02841957974580122\n",
            "Training loss per 100 training steps: 0.028303032555214606\n",
            "Training loss per 100 training steps: 0.028208665607398972\n",
            "Training loss per 100 training steps: 0.02812556893842179\n",
            "Training loss per 100 training steps: 0.027987396835758068\n",
            "Training loss per 100 training steps: 0.027855905963296367\n",
            "Training loss per 100 training steps: 0.027763965625289502\n",
            "Training loss per 100 training steps: 0.027683528937909374\n",
            "Training loss per 100 training steps: 0.027602986056977283\n",
            "Training loss per 100 training steps: 0.02750559658860346\n",
            "Training loss per 100 training steps: 0.02737329748926413\n",
            "Training loss per 100 training steps: 0.027246929948375517\n",
            "Training loss per 100 training steps: 0.02711390869051137\n",
            "Training loss per 100 training steps: 0.02702058333694825\n",
            "Training loss per 100 training steps: 0.02691732426450559\n",
            "Training loss per 100 training steps: 0.026855590169888054\n",
            "Training loss per 100 training steps: 0.026747399695828487\n",
            "Training loss per 100 training steps: 0.026649140221165022\n",
            "Training loss per 100 training steps: 0.026564663422749355\n",
            "Training loss per 100 training steps: 0.02648956814602031\n",
            "Training loss per 100 training steps: 0.026403372765754055\n",
            "Training loss per 100 training steps: 0.026326268705972305\n",
            "Training loss per 100 training steps: 0.026262537005149595\n",
            "Training loss per 100 training steps: 0.026183499005300825\n",
            "Training loss epoch: 0.0261348150214793\n",
            "Training accuracy epoch: 0.9332101148067913\n"
          ]
        }
      ],
      "source": [
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "        loss, tr_logits = outputs.loss, outputs.logits\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "\n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "\n",
        "        # compute training accuracy\n",
        "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        active_accuracy = mask.view(-1) == 1 # shape (batch_size * seq_len,)\n",
        "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "        tr_preds.extend(predictions)\n",
        "        tr_labels.extend(targets)\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEWMF9PlLXmf",
        "outputId": "c32dda19-0d3d-43a7-b28a-56223c19dc71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss per 100 evaluation steps: 0.05145786702632904\n",
            "Invalid tokenized sentence or labels at index 1245. Skipping entry.\n",
            "Validation loss per 100 evaluation steps: 0.0193598467515468\n",
            "Validation loss per 100 evaluation steps: 0.019549503674573122\n",
            "Validation loss per 100 evaluation steps: 0.01935605811221259\n",
            "Validation loss per 100 evaluation steps: 0.019305186031474512\n",
            "Validation Loss: 0.01910663925689522\n",
            "Validation Accuracy: 0.9503086591693927\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            if batch is None:  # Skip invalid batches\n",
        "                continue\n",
        "\n",
        "            ids = batch['ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['mask'].to(device, dtype=torch.long)\n",
        "            targets = batch['targets'].to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss, eval_logits = outputs.loss, outputs.logits\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += targets.size(0)\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                print(f\"Validation loss per 100 evaluation steps: {eval_loss / nb_eval_steps}\")\n",
        "\n",
        "            # Compute accuracy\n",
        "            flattened_targets = targets.view(-1)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1)\n",
        "            active_accuracy = mask.view(-1) == 1\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "            eval_labels.extend(targets)\n",
        "            eval_preds.extend(predictions)\n",
        "\n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    eval_loss /= nb_eval_steps\n",
        "    eval_accuracy /= nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return eval_labels, eval_preds\n",
        "labels, predictions = valid(model, testing_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmoZ9WWIXXCR"
      },
      "outputs": [],
      "source": [
        "for param in model.parameters(): param.data = param.data.contiguous()\n",
        "model.save_pretrained('/content/drive/MyDrive/Kasre_Ez/models_5000')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpaVXqFGhBXL",
        "outputId": "0b8b3603-a2ae-438a-af64-1136b66074c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Kasre_Ez/models_5000/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Kasre_Ez/models_5000/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Kasre_Ez/models_5000/vocab.txt',\n",
              " '/content/drive/MyDrive/Kasre_Ez/models_5000/added_tokens.json',\n",
              " '/content/drive/MyDrive/Kasre_Ez/models_5000/tokenizer.json')"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.save_pretrained('/content/drive/MyDrive/Kasre_Ez/models_5000')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XzTOppmjCDe"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "model_name = \"/content/drive/MyDrive/Kasre_Ez/models_folder\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oU41d4eWqPB",
        "outputId": "08e7bceb-c667-47ab-80cd-8a22d6e9dad9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        WORD       0.84      0.87      0.85     26548\n",
            "\n",
            "   micro avg       0.84      0.87      0.85     26548\n",
            "   macro avg       0.84      0.87      0.85     26548\n",
            "weighted avg       0.84      0.87      0.85     26548\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from seqeval.metrics import classification_report\n",
        "labels = [label.item() for label in labels]  # If labels are tensors\n",
        "predictions = [prediction.item() for prediction in predictions]  # If predictions are tensors\n",
        "\n",
        "labels = [id2label[label_id] for label_id in labels]  # Mapping label IDs to label names\n",
        "predictions = [id2label[pred_id] for pred_id in predictions]  # Mapping prediction IDs to label names\n",
        "\n",
        "print(classification_report([labels], [predictions]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jfwLkKVSrNir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Fine_tune Again"
      ],
      "metadata": {
        "id": "P0x1ukcJrRxD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dba8a52-a413-430a-da8f-089e4c228699",
        "id": "o-2FDZ2uyK50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 0.01758834533393383\n",
            "Training loss per 100 training steps: 0.01977159732971156\n",
            "Training loss epoch: 0.018722230637649773\n",
            "Training accuracy epoch: 0.9563997475992332\n"
          ]
        }
      ],
      "source": [
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "\n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "        loss, tr_logits = outputs.loss, outputs.logits\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "\n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "\n",
        "        # compute training accuracy\n",
        "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        active_accuracy = mask.view(-1) == 1 # shape (batch_size * seq_len,)\n",
        "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "        tr_preds.extend(predictions)\n",
        "        tr_labels.extend(targets)\n",
        "\n",
        "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oPkUxedQyMEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad0bf234-d6d7-43be-900a-76c3d4b05da3",
        "id": "7gMCR_PWybsk"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss per 100 evaluation steps: 0.00766481040045619\n",
            "Validation Loss: 0.015229172247927636\n",
            "Validation Accuracy: 0.9601837151280009\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "def valid(model, testing_loader):\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            if batch is None:  # Skip invalid batches\n",
        "                continue\n",
        "\n",
        "            ids = batch['ids'].to(device, dtype=torch.long)\n",
        "            mask = batch['mask'].to(device, dtype=torch.long)\n",
        "            targets = batch['targets'].to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss, eval_logits = outputs.loss, outputs.logits\n",
        "\n",
        "            eval_loss += loss.item()\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += targets.size(0)\n",
        "\n",
        "            if idx % 100 == 0:\n",
        "                print(f\"Validation loss per 100 evaluation steps: {eval_loss / nb_eval_steps}\")\n",
        "\n",
        "            # Compute accuracy\n",
        "            flattened_targets = targets.view(-1)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1)\n",
        "            active_accuracy = mask.view(-1) == 1\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "\n",
        "            eval_labels.extend(targets)\n",
        "            eval_preds.extend(predictions)\n",
        "\n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    eval_loss /= nb_eval_steps\n",
        "    eval_accuracy /= nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return eval_labels, eval_preds\n",
        "labels, predictions = valid(model, testing_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48a9558a-ab4d-447a-d41e-5f7275f1d141",
        "id": "FyUeE5_RykSg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        WORD       0.87      0.82      0.84       337\n",
            "\n",
            "   micro avg       0.87      0.82      0.84       337\n",
            "   macro avg       0.87      0.82      0.84       337\n",
            "weighted avg       0.87      0.82      0.84       337\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#results of finetuned twice model\n",
        "from seqeval.metrics import classification_report\n",
        "labels = [label.item() for label in labels]  # If labels are tensors\n",
        "predictions = [prediction.item() for prediction in predictions]  # If predictions are tensors\n",
        "\n",
        "labels = [id2label[label_id] for label_id in labels]  # Mapping label IDs to label names\n",
        "predictions = [id2label[pred_id] for pred_id in predictions]  # Mapping prediction IDs to label names\n",
        "\n",
        "print(classification_report([labels], [predictions]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "758f2385-1c2f-4667-911f-c36372f923ec",
        "id": "VP-PWqhzrNot"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        WORD       0.72      0.90      0.80       337\n",
            "\n",
            "   micro avg       0.72      0.90      0.80       337\n",
            "   macro avg       0.72      0.90      0.80       337\n",
            "weighted avg       0.72      0.90      0.80       337\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Results of first model\n",
        "from seqeval.metrics import classification_report\n",
        "labels = [label.item() for label in labels]  # If labels are tensors\n",
        "predictions = [prediction.item() for prediction in predictions]  # If predictions are tensors\n",
        "\n",
        "labels = [id2label[label_id] for label_id in labels]  # Mapping label IDs to label names\n",
        "predictions = [id2label[pred_id] for pred_id in predictions]  # Mapping prediction IDs to label names\n",
        "\n",
        "print(classification_report([labels], [predictions]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SwwwUpEtrjJy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}